{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzGO71rnrehXa6Jq6+Pan8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujatayal/Colab-Notebooks/blob/main/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcm2GpQU9aZm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "7c02350e-5d35-4abc-a9bd-a0c642a9af92"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from tensorflow.keras.models import  Model\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.9MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30539 sha256=ee03ffce3e12b64bf96da9c1ec0601b6bc660f67a16a36d9293c186c0133d4e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7304 sha256=b87bc5da244905a3f5e380c3d81791588058539ca3f5d5c5019793aaf13600fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=889a11296bd9895ea8ec845de4d50a52062c0145a4f9b65e64bf2411f2c9ece5\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "TensorFlow Version: 2.3.0\n",
            "Hub version:  0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qz_xsycHVno"
      },
      "source": [
        "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n",
        "#https://androidkt.com/simple-text-classification-using-bert-in-tensorflow-keras-2-0/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20-zxnfgHc1X"
      },
      "source": [
        "MAX_SEQ_LEN=128\n",
        "input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2SoC61bHnyo"
      },
      "source": [
        "#input token ids is tokenizer converts tokens using vocab file.\n",
        "#input masks are either 0 or 1. 1 for useful tokens, 0 for padding.\n",
        "#segment ids are either 0 or 1. For 2 text training: 0 for the first one, 1 for the second one.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-PO0NNZHutI"
      },
      "source": [
        "def get_masks(tokens, max_seq_length):\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pf5Eg7ZHuwq"
      },
      "source": [
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "#pooled_output representations the entire input sequences and sequence_output representations each input token in the context.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5idotbYnHu3t"
      },
      "source": [
        "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
        "\n",
        "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "\n",
        "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjb1ek1THu09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "87e3f0da-7ad2-4ade-c4fa-77b1c4097478"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"anujatayal\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"4566267aec1fb886b205af0ce951f202\" # key from the json file\n",
        "/content/sample_data/train.csv!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading jigsaw-toxic-comment-classification-challenge.zip to /content\n",
            " 78% 41.0M/52.6M [00:01<00:00, 21.8MB/s]\n",
            "100% 52.6M/52.6M [00:01<00:00, 33.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKIXdAvbIOCH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "8812f2b8-bfc1-4918-934e-5a46d75bba70"
      },
      "source": [
        "pip install kaggle --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3XJDCv6I7gs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63b69dd1-c900-4b08-a243-6d0de28d8f21"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv('/content/sample_data/train.csv',engine='python',error_bad_lines=False)\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "train_sentences = df1[\"comment_text\"].fillna(\"CVxTz\").values\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "train_y = df1[list_classes].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 31600: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQq9Udi8NYH4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "468ddf4f-78ac-4f57-dbe0-4cff64f28d43"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15497</th>\n",
              "      <td>28e41d194c823996</td>\n",
              "      <td>\"\\n\\n Harassment \\n\\nPlease do not Out contrib...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7962</th>\n",
              "      <td>1534e164eee103a1</td>\n",
              "      <td>Hey, dumbfuck, quit fucking around with Wikipe...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20723</th>\n",
              "      <td>36b6cd63eb7cac9f</td>\n",
              "      <td>The comments that have been made seem to make ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16565</th>\n",
              "      <td>2baf211e95de7aac</td>\n",
              "      <td>Revert Again -Yes, LGagnon is well-known throu...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16367</th>\n",
              "      <td>2b22e0ffb5b65ece</td>\n",
              "      <td>\"\\n\"\"Vandalism\"\"? Sorry, mister, but I haven't...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     id  ... identity_hate\n",
              "15497  28e41d194c823996  ...             0\n",
              "7962   1534e164eee103a1  ...             0\n",
              "20723  36b6cd63eb7cac9f  ...             0\n",
              "16565  2baf211e95de7aac  ...             0\n",
              "16367  2b22e0ffb5b65ece  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2KmekGXLFqL"
      },
      "source": [
        "def create_single_input(sentence,MAX_LEN):\n",
        "  stokens = tokenizer.tokenize(sentence)\n",
        "  stokens = stokens[:MAX_LEN]\n",
        "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
        "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
        "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
        "  return ids,masks,segments\n",
        "\n",
        "def create_input_array(sentences):\n",
        "  input_ids, input_masks, input_segments = [], [], []\n",
        "  for sentence in tqdm(sentences,position=0, leave=True):\n",
        "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n",
        "    input_ids.append(ids)\n",
        "    input_masks.append(masks)\n",
        "    input_segments.append(segments)\n",
        "\n",
        "  return [np.asarray(input_ids, dtype=np.int32), \n",
        "            np.asarray(input_masks, dtype=np.int32), \n",
        "            np.asarray(input_segments, dtype=np.int32)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uw5DZdnNcTH"
      },
      "source": [
        "x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "out = tf.keras.layers.Dense(6, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
        "\n",
        "model = tf.keras.models.Model(\n",
        "      inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYy8hUavNw7H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3f470317-8f4a-4fbf-f48b-3f7364b8b3e6"
      },
      "source": [
        "inputs=create_input_array(train_sentences)\n",
        "\n",
        "model.fit(inputs,train_y,epochs=1,batch_size=32,validation_split=0.2,shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:05<00:00, 937.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 132s 1s/step - loss: 0.1663 - accuracy: 0.6768 - val_loss: 0.1679 - val_accuracy: 0.9950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd262913c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caXx2CM2N7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "outputId": "10c25046-40aa-4295-d3e7-82e17b7f1f92"
      },
      "source": [
        "test_df=pd.read_csv(\"/content/sample_data/test.csv\")\n",
        "test_sentences = test_df[\"comment_text\"].fillna(\"CVxTz\").values\n",
        "test_inputs=create_input_array(test_sentences[110:150])\n",
        "print(model.predict(test_inputs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:00<00:00, 603.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.05256845 0.00570976 0.01252703 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252703 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252703 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252703 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252703 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252703 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252703 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252703 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252703 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252703 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246803 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252703 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252703 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256847 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]\n",
            " [0.05256845 0.00570976 0.01252702 0.00219777 0.03246804 0.0058945 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjELvwaQOVKu"
      },
      "source": [
        "df1=df[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjM1fSgXOVOn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62b0befd-35eb-4b7a-cfb1-f22cb30a9a8e"
      },
      "source": [
        "len(df1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6DgFGHJOeoU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}